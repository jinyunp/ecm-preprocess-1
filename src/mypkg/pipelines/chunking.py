#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Chunk ECMiner manual JSON (paragraphs/tables/inline_images) into
section-based chunks suitable for vector DB ingestion.

- Chunk unit: heading level = CHUNK_LEVEL (default: 3)
- Section prefix sentences are injected before actual content.
- Media IDs (Image_ids, Table_ids) are extracted from context text by
  scanning [Image:...] / [Table:...] markers.

Input JSON format (simplified):
{
  "paragraphs": [
    {
      "text": "Chapter 1 ECMinerâ„¢ Overview",
      "doc_index": 3,
      "style": "heading 1",
      "source_doc_indices": [3],
      ...
    },
    ...
  ],
  "tables": [
    {
      "tid": "t21",
      "doc_index": 21,
      "table_html": "<table>...</table>",
      ...
    },
    ...
  ],
  "inline_images": [
    {
      "rId": "rId26",
      "doc_index": 65,
      "ocr_text": "...",
      ...
    },
    ...
  ]
}
"""

import json
import os
import re
import sys
from typing import Any, Dict, List, Optional, Tuple


# === CONFIGURABLE PARTS =====================================================

# ê¸°ì¤€ì´ ë˜ëŠ” heading levelë“¤ (heading 2, heading 3ì—ì„œ ëª¨ë‘ chunk ì‹œì‘)
CHUNK_LEVELS = {2, 3}

# style -> heading level ë§¤í•‘ (í•„ìš”í•  ë•Œ ììœ ë¡­ê²Œ ìˆ˜ì •)
STYLE_LEVEL_MAP = {
    "heading 1": 1,
    "heading1": 1,
    "heading 2": 2,
    "heading2": 2,
    "heading 3": 3,
    "heading3": 3,
    # "ì†Œì œëª©2"ê°€ í¬í•¨ë˜ëŠ” ìŠ¤íƒ€ì¼ì€ level 4ë¡œ ì·¨ê¸‰
    "ì†Œì œëª©2": 4,
    "subtitle2": 4,
}


# === HELPER FUNCTIONS =======================================================

def normalize_style(style: Optional[str]) -> str:
    if not style:
        return ""
    return style.strip().lower()


def guess_heading_level(style: Optional[str], text: str) -> Tuple[Optional[int], bool]:
    """
    styleê³¼ textë¥¼ ë³´ê³  heading levelì„ ì¶”ì •í•œë‹¤.
    ë°˜í™˜: (level, is_heading)
    """
    s = normalize_style(style)

    # 1) style ê¸°ë°˜ ë§¤í•‘
    for key, lvl in STYLE_LEVEL_MAP.items():
        if key in s:
            return lvl, True

    # 2) text íŒ¨í„´ ê¸°ë°˜ (fallback)
    t = text.strip()

    # "Chapter 1 ECMinerâ„¢ Overview" ê°™ì€ ê²½ìš°
    if re.match(r"^Chapter\s+\d+\b", t, flags=re.IGNORECASE):
        return 1, True

    # "1", "1.1", "1.1.1" + ê³µë°± + ì œëª©
    if re.match(r"^\d+(\.\d+)*\s+", t):
        # ê¹Šì´ì— ë”°ë¼ level ì¶”ì • (ììœ ë¡­ê²Œ ì¡°ì • ê°€ëŠ¥)
        depth = t.split()[0].count(".") + 1
        return min(depth, 4), True

    return None, False


def clean_heading_text(text: str) -> str:
    """
    Section prefixì— ë„£ê¸° ì „ì— "Chapter #", "#.#" ê°™ì€ ë²ˆí˜¸ë¥¼ ì œê±°.
    ì˜ˆ:
      "Chapter 1 ECMinerâ„¢ Overview" -> "ECMinerâ„¢ Overview"
      "1.1.1 Test Section" -> "Test Section"
    """
    t = text.strip()

    # "Chapter 1 xxx"
    t = re.sub(r"^Chapter\s+\d+\s+", "", t, flags=re.IGNORECASE)

    # "1.1.1 " or "1 " ë“± ë²ˆí˜¸ ì œê±°
    t = re.sub(r"^\d+(\.\d+)*\s+", "", t)

    return t.strip()


def build_section_intro(section_path: List[str]) -> str:
    """
    section_path: ["L1", "L2", "L3", "L4?"] í˜•íƒœì˜ ì œëª© ë¦¬ìŠ¤íŠ¸.
    ê¸¸ì´ì— ë”°ë¼ ë‹¤ë¥¸ í…œí”Œë¦¿ìœ¼ë¡œ section prefix ë¬¸ì¥ êµ¬ì„±.
    ì—¬ê¸°ì„œëŠ” ë¬¸ì¥ì— ë„£ê¸° ì „ì— ë²ˆí˜¸(Chapter, 1.1, 1.1.1 ë“±)ë¥¼ ì œê±°í•œë‹¤.
    """
    # ë¹ˆ ê°’ ì œê±° + ë¬¸ì¥ìš©ìœ¼ë¡œ ë²ˆí˜¸ ì œê±°
    titles = [clean_heading_text(t) for t in section_path if t]

    if not titles:
        return ""

    if len(titles) == 3:
        L1, L2, L3 = titles
        return (
            f"This content is about the section {L1}, "
            f"and more specifically it belongs to {L2}, focusing on {L3}."
        )
    elif len(titles) >= 4:
        L1, L2, L3, L4 = titles[:4]
        return (
            f"This content is about the section {L1}, "
            f"and more specifically it belongs to {L2}, "
            f"under the subsection {L3}, with a detailed reference to {L4}."
        )
    else:
        # level 1 or 2ë§Œ ìˆëŠ” ê²½ìš°ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if len(titles) == 1:
            return f"This content is about the section {titles[0]}."
        elif len(titles) == 2:
            return (
                f"This content is about the section {titles[0]}, "
                f"and more specifically it belongs to {titles[1]}."
            )
        return ""



def extract_media_ids_from_text(text: str) -> Tuple[List[str], List[str]]:
    """
    Context í…ìŠ¤íŠ¸ ì•ˆì—ì„œ [Image:rId##], [Table:tid] íŒ¨í„´ì„ ì°¾ì•„
    Image_ids, Table_ids ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ë°˜í™˜.
    """
    image_ids = re.findall(r"\[Image:([^\]]+)\]", text)
    table_ids = re.findall(r"\[Table:([^\]]+)\]", text)

    # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
    def unique(seq: List[str]) -> List[str]:
        seen = set()
        out = []
        for x in seq:
            if x not in seen:
                seen.add(x)
                out.append(x)
        return out

    return unique(image_ids), unique(table_ids)

def get_default_output_path(input_path: str) -> str:
    """
    ì…ë ¥: .../_sanitized/1ì¥_v3.1_sanitized.json
    ì¶œë ¥: .../_chunked/1ì¥_v3.1_chunked.json
    """
    dirpath, filename = os.path.split(input_path)
    name, ext = os.path.splitext(filename)

    # íŒŒì¼ëª…ì—ì„œ _sanitized ì œê±° í›„ _chunkedë¡œ ë³€ê²½
    if name.endswith("_sanitized"):
        base_name = name[: -len("_sanitized")]
    else:
        base_name = name
    out_filename = base_name + "_chunked" + ext

    # ë””ë ‰í„°ë¦¬ì—ì„œ _sanitized â†’ _chunkedë¡œ ë³€ê²½
    parent_dir, last_dir = os.path.split(dirpath)
    if last_dir == "_sanitized":
        out_dir = os.path.join(parent_dir, "_chunked")
    else:
        # fallback: ê·¸ëƒ¥ ê°™ì€ ë””ë ‰í„°ë¦¬ì— ì €ì¥
        out_dir = dirpath

    os.makedirs(out_dir, exist_ok=True)
    return os.path.join(out_dir, out_filename)


# === CORE CHUNKING LOGIC ====================================================

def build_elements(doc: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    paragraphs, tables, inline_imagesë¥¼ doc_index ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ í•©ì¹œë‹¤.
    ê° element: {"kind": "paragraph"|"table"|"image", "doc_index": int, "obj": original_dict}
    """
    elements: List[Dict[str, Any]] = []

    for p in doc.get("paragraphs", []):
        elements.append(
            {"kind": "paragraph", "doc_index": p.get("doc_index", 0), "obj": p}
        )

    for t in doc.get("tables", []):
        elements.append(
            {"kind": "table", "doc_index": t.get("doc_index", 0), "obj": t}
        )

    for im in doc.get("inline_images", []):
        elements.append(
            {"kind": "image", "doc_index": im.get("doc_index", 0), "obj": im}
        )

    elements.sort(key=lambda e: e["doc_index"])
    return elements


def chunk_document(doc: Dict[str, Any], file_name: str) -> List[Dict[str, Any]]:
    """
    ë©”ì¸ chunking í•¨ìˆ˜.
    ë°˜í™˜: chunk ë¦¬ìŠ¤íŠ¸ (ê° chunkëŠ” metadata í•„ë“œ í¬í•¨)
    """
    elements = build_elements(doc)

    # level -> section title (cleaned)
    section_titles: Dict[int, str] = {}

    chunks: List[Dict[str, Any]] = []
    current_chunk: Dict[str, Any] = {}
    current_texts: List[str] = []
    current_pages: List[int] = []

    def flush_current_chunk():
        nonlocal current_chunk, current_texts, current_pages, chunks

        if not current_chunk:
            return

        intro = current_chunk.get("section_intro", "")
        body = "\n\n".join([t for t in current_texts if t.strip()])

        if intro and body:
            context = intro + "\n\n" + body
        elif intro:
            context = intro
        else:
            context = body

        image_ids, table_ids = extract_media_ids_from_text(context)

        chunk_meta = {
            "Context": context,
            "Context_id": current_chunk["context_id"],
            "Is_image": bool(image_ids),
            "Image_ids": image_ids,
            "Is_table": bool(table_ids),
            "Table_ids": table_ids,
            "Section_path": current_chunk["section_path"],
            "Section_length": len(
                [t for t in current_chunk["section_path"] if t]
            ),
            "Page_number": min(current_pages) if current_pages else None,
            "File_name": file_name,
        }

        chunks.append(chunk_meta)

        # reset
        current_chunk = {}
        current_texts = []
        current_pages = []

    # chunk ID ì¦ê°€ìš©
    chunk_counter = 0

    for el in elements:
        kind = el["kind"]
        obj = el["obj"]

        if kind == "paragraph":
            text = obj.get("text", "") or ""
            style = obj.get("style")
            page = obj.get("page_number")  # ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None

            level, is_heading = guess_heading_level(style, text)

            if is_heading:
                # section path ì—…ë°ì´íŠ¸
                if level is not None:
                    section_titles[level] = text.strip()
                    # ë” ê¹Šì€ ë ˆë²¨ ì´ˆê¸°í™”
                    for k in list(section_titles.keys()):
                        if k > level:
                            del section_titles[k]

                # ğŸ” UPDATED: heading 2, 3, ê·¸ë¦¬ê³  ì†Œì œëª©2(level 4)ì—ì„œ ìƒˆ chunk ì‹œì‘
                if level in CHUNK_LEVELS or level == 4:
                    flush_current_chunk()

                    chunk_counter += 1
                    # í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  ìƒìœ„ ì œëª©ì„ pathë¡œ ì‚¬ìš© (1,2,3,4...)
                    path_levels = sorted(section_titles.keys())
                    section_path = [section_titles[lvl] for lvl in path_levels]
                    section_intro = build_section_intro(section_path)

                    current_chunk = {
                        "context_id": f"{os.path.splitext(file_name)[0]}::chunk_{chunk_counter:04d}",
                        "section_path": section_path,
                        "section_intro": section_intro,
                    }
                    current_texts = []
                    current_pages = []

                # heading ë¬¸ë‹¨ ìì²´ëŠ” body í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ
                continue

            # ì¼ë°˜ ë¬¸ë‹¨ (heading ì•„ë‹˜)
            if not current_chunk:
                # ğŸ” UPDATED: ì•„ì§ chunkê°€ ì—†ê³ , Chapter(heading1)ë§Œ ìˆëŠ” ê²½ìš°ì—ëŠ”
                #            heading1 ê¸°ì¤€ìœ¼ë¡œ introë¥¼ ê°€ì§„ chunkë¥¼ ì‹œì‘
                available_levels = [lvl for lvl in section_titles.keys() if lvl in CHUNK_LEVELS]
                if not available_levels and 1 in section_titles:
                    base_level = 1

                    chunk_counter += 1
                    path_levels = sorted(
                        [lvl for lvl in section_titles.keys() if lvl <= base_level]
                    )
                    section_path = [section_titles[lvl] for lvl in path_levels]
                    section_intro = build_section_intro(section_path)

                    current_chunk = {
                        "context_id": f"{os.path.splitext(file_name)[0]}::chunk_{chunk_counter:04d}",
                        "section_path": section_path,
                        "section_intro": section_intro,
                    }
                    current_texts = []
                    current_pages = []
                else:
                    # heading2/3 ê¸°ë°˜ chunkê°€ ì´ë¯¸ ìƒì„±ë˜ì–´ì•¼ í•˜ëŠ” ìƒí™©ì¸ë°
                    # ì•„ì§ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ìŠ¤í‚µ
                    continue

            if text.strip():
                current_texts.append(text)

            if page is not None:
                current_pages.append(page)


        elif kind == "table":
            # ì¼ë°˜ ë¬¸ë‹¨ (heading ì•„ë‹˜)
            if not current_chunk:
                # ì•„ì§ chunk ì‹œì‘ ì „ì´ë¼ë©´ ìŠ¤í‚µ
                continue

            table = obj
            tid = table.get("tid")
            page = table.get("page_number")

            if tid:
                current_texts.append(f"[Table:{tid}]")
            if page is not None:
                current_pages.append(page)
                
        elif kind == "image":
            if not current_chunk:
                # ì•„ì§ chunk ì‹œì‘ ì „ì´ë©´ ìŠ¤í‚µ
                continue

            im = obj
            rid = im.get("rId")
            page = im.get("page_number")

            if rid:
                current_texts.append(f"[Image:{rid}]")
            if page is not None:
                current_pages.append(page)


    # ë§ˆì§€ë§‰ chunk flush
    flush_current_chunk()

    return chunks


# === MAIN ===================================================================

def main(input_path: str, output_path: Optional[str] = None):
    with open(input_path, "r", encoding="utf-8") as f:
        doc = json.load(f)

    file_name = os.path.basename(input_path)

    chunks = chunk_document(doc, file_name=file_name)

    if not output_path:
        # UPDATED: sanitized â†’ chunked ë””ë ‰í„°ë¦¬ ë° íŒŒì¼ëª… ìë™ ìƒì„±
        output_path = get_default_output_path(input_path)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)

    print(f"Saved {len(chunks)} chunks to {output_path}")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python chunk_manual.py <input_json_path> [output_json_path]")
        sys.exit(1)

    in_path = sys.argv[1]
    out_path = sys.argv[2] if len(sys.argv) >= 3 else None
    main(in_path, out_path)
    
# python src/mypkg/pipelines/chunking.py /home/jinypark/vscodeProjects/ecm-preprocess-1/output/processed/Appendix_v2/v0/_sanitized/Appendix_v2_sanitized.json

# python src/mypkg/pipelines/chunking.py /home/jinypark/vscodeProjects/ecm-preprocess-1/output/processed/1ì¥_v3.1/v0/_sanitized/1ì¥_v3.1_sanitized.json